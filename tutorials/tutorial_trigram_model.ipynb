{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(words))\n",
    "test_size = int(0.1 * len(words))\n",
    "dev_size = len(words) - (train_size+test_size)\n",
    "train, dev, test = torch.utils.data.random_split(words, [train_size, dev_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, 25626, 3203, 3204)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(train), len(test), len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27*27,27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words)))) # getting the list of all unique characters in the dataset in sorted order(a-z)\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] # adding the special start and end characters\n",
    "    for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[27*ix1+ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 27])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-410414.9688)\n",
      "nll = 410414.96875\n",
      "norm nll = 2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        prob = P[27*ix1+ix2, ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "print(f\"nll = {-log_likelihood}\")\n",
    "print(f\"norm nll = {-log_likelihood/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
       "        [0.0002, 0.0469, 0.0430,  ..., 0.0063, 0.0392, 0.0345],\n",
       "        [0.0008, 0.1275, 0.0008,  ..., 0.0008, 0.0038, 0.0008],\n",
       "        ...,\n",
       "        [0.0714, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],\n",
       "        [0.2011, 0.1609, 0.0057,  ..., 0.0057, 0.0057, 0.0115],\n",
       "        [0.0694, 0.1944, 0.0139,  ..., 0.0139, 0.1111, 0.0139]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    row_idx = random.randint(1,26)\n",
    "    print(row_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khur.\n",
      "daxx.\n",
      "grenavissyn.\n",
      "jobhon.\n",
      "wikataib.\n",
      "khdivery.\n",
      "asie.\n",
      "zia.\n",
      "ron.\n",
      "gel.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "random.seed(2147483647)\n",
    "names = []\n",
    "\n",
    "for i in range(10):\n",
    "    row_idx = random.randint(1,26)\n",
    "    char = itos[row_idx] \n",
    "    name = char\n",
    "    idx1 = row_idx\n",
    "    while char != '.':\n",
    "        p = P[row_idx, :]\n",
    "        idx2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        # print(idx2)\n",
    "        char = itos[idx2]\n",
    "        # print(char)\n",
    "        name += char\n",
    "        row_idx = 27*idx1+idx2\n",
    "        idx1 = idx2\n",
    "    print(name)\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e -> m\n",
      "em -> m\n",
      "mm -> a\n",
      "ma -> .\n"
     ]
    }
   ],
   "source": [
    "# creating the training set of examples\n",
    "xs , ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1,ix2))\n",
    "        ys.append(ix3)\n",
    "        print(f'{ch1}{ch2} -> {ch3}')\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  5],\n",
       "         [ 5, 13],\n",
       "         [13, 13],\n",
       "         [13,  1]]),\n",
       " tensor([13, 13,  1,  0]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "num_classes = 27\n",
    "xenc = torch.cat([F.one_hot(xs[:, 0], num_classes=num_classes),\n",
    "                  F.one_hot(xs[:, 1], num_classes=num_classes)], dim=1).float() # casting to float as nn work with continuous numeric inputs\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 54])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe1ded2a8b0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5978, 0.3356, 0.5786, 0.8807, 0.5856, 1.7991, 0.6113, 0.3452, 1.1852,\n",
       "         0.7767, 0.3590, 1.1629, 0.8756, 1.0262, 1.2016, 0.6726, 1.0825, 0.8145,\n",
       "         1.0160, 1.0780, 1.2979, 0.9438, 1.7528, 0.5156, 1.9046, 1.9612, 0.8580],\n",
       "        [0.5437, 1.2590, 0.7740, 1.1043, 0.8463, 0.2554, 1.3184, 0.7464, 1.9034,\n",
       "         0.4161, 1.5267, 1.4233, 1.0296, 1.1407, 1.7127, 0.6222, 1.6935, 0.6522,\n",
       "         1.2033, 0.5601, 1.4115, 0.7167, 1.1423, 1.2992, 1.6432, 0.1882, 0.3667],\n",
       "        [1.1100, 1.5333, 0.3809, 0.4420, 1.0938, 1.1458, 1.3691, 1.3634, 1.7844,\n",
       "         0.8290, 1.4311, 1.3325, 0.4111, 1.2367, 1.7418, 1.0438, 1.2482, 1.2467,\n",
       "         0.2838, 0.9503, 0.9053, 0.7962, 0.2923, 0.8774, 0.9498, 0.6804, 0.5005],\n",
       "        [1.8115, 1.5663, 0.8463, 0.3307, 1.3895, 1.4165, 0.6767, 0.8702, 1.1565,\n",
       "         1.1369, 0.8956, 0.8232, 0.4459, 0.6501, 0.9696, 0.9515, 0.6746, 0.9228,\n",
       "         0.1355, 1.2152, 1.1566, 0.8740, 0.4955, 0.7112, 0.6295, 1.4023, 0.6585]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating 27 neurons with weights instialized randomly\n",
    "W = torch.rand((54,27))\n",
    "(xenc @ W) # (5x54) @ (54x27) = (5x27) (for each input activation of all 27 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0228, 0.0175, 0.0223, 0.0302, 0.0225, 0.0757, 0.0231, 0.0177, 0.0410,\n",
       "         0.0272, 0.0179, 0.0401, 0.0301, 0.0350, 0.0417, 0.0245, 0.0370, 0.0283,\n",
       "         0.0346, 0.0368, 0.0459, 0.0322, 0.0723, 0.0210, 0.0841, 0.0890, 0.0295],\n",
       "        [0.0206, 0.0422, 0.0260, 0.0362, 0.0279, 0.0155, 0.0448, 0.0253, 0.0804,\n",
       "         0.0182, 0.0552, 0.0498, 0.0336, 0.0375, 0.0665, 0.0223, 0.0652, 0.0230,\n",
       "         0.0399, 0.0210, 0.0492, 0.0245, 0.0376, 0.0439, 0.0620, 0.0145, 0.0173],\n",
       "        [0.0380, 0.0580, 0.0183, 0.0195, 0.0374, 0.0394, 0.0492, 0.0489, 0.0745,\n",
       "         0.0287, 0.0524, 0.0474, 0.0189, 0.0431, 0.0714, 0.0355, 0.0436, 0.0435,\n",
       "         0.0166, 0.0324, 0.0309, 0.0277, 0.0168, 0.0301, 0.0324, 0.0247, 0.0206],\n",
       "        [0.0839, 0.0656, 0.0319, 0.0191, 0.0550, 0.0565, 0.0270, 0.0327, 0.0436,\n",
       "         0.0427, 0.0336, 0.0312, 0.0214, 0.0263, 0.0361, 0.0355, 0.0269, 0.0345,\n",
       "         0.0157, 0.0462, 0.0436, 0.0328, 0.0225, 0.0279, 0.0257, 0.0557, 0.0265]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (xenc @ W) # log-counts\n",
    "counts = logits.exp() # equivalent N\n",
    "probs = counts / torch.sum(counts, 1, keepdim=True) # probabilities for the next character\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .em (indexes 0,5,13)\n",
      "input to the neural net: (0, 5)\n",
      "output probabilities from the neural net: tensor([0.0228, 0.0175, 0.0223, 0.0302, 0.0225, 0.0757, 0.0231, 0.0177, 0.0410,\n",
      "        0.0272, 0.0179, 0.0401, 0.0301, 0.0350, 0.0417, 0.0245, 0.0370, 0.0283,\n",
      "        0.0346, 0.0368, 0.0459, 0.0322, 0.0723, 0.0210, 0.0841, 0.0890, 0.0295])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.03495420888066292\n",
      "log likelihood: -3.3537163734436035\n",
      "negative log likelihood: 3.3537163734436035\n",
      "--------\n",
      "bigram example 2: emm (indexes 5,13,13)\n",
      "input to the neural net: (5, 13)\n",
      "output probabilities from the neural net: tensor([0.0206, 0.0422, 0.0260, 0.0362, 0.0279, 0.0155, 0.0448, 0.0253, 0.0804,\n",
      "        0.0182, 0.0552, 0.0498, 0.0336, 0.0375, 0.0665, 0.0223, 0.0652, 0.0230,\n",
      "        0.0399, 0.0210, 0.0492, 0.0245, 0.0376, 0.0439, 0.0620, 0.0145, 0.0173])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.037507593631744385\n",
      "log likelihood: -3.2832119464874268\n",
      "negative log likelihood: 3.2832119464874268\n",
      "--------\n",
      "bigram example 3: mma (indexes 13,13,1)\n",
      "input to the neural net: (13, 13)\n",
      "output probabilities from the neural net: tensor([0.0380, 0.0580, 0.0183, 0.0195, 0.0374, 0.0394, 0.0492, 0.0489, 0.0745,\n",
      "        0.0287, 0.0524, 0.0474, 0.0189, 0.0431, 0.0714, 0.0355, 0.0436, 0.0435,\n",
      "        0.0166, 0.0324, 0.0309, 0.0277, 0.0168, 0.0301, 0.0324, 0.0247, 0.0206])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.05798986926674843\n",
      "log likelihood: -2.847486972808838\n",
      "negative log likelihood: 2.847486972808838\n",
      "--------\n",
      "bigram example 4: ma. (indexes 13,1,0)\n",
      "input to the neural net: (13, 1)\n",
      "output probabilities from the neural net: tensor([0.0839, 0.0656, 0.0319, 0.0191, 0.0550, 0.0565, 0.0270, 0.0327, 0.0436,\n",
      "        0.0427, 0.0336, 0.0312, 0.0214, 0.0263, 0.0361, 0.0355, 0.0269, 0.0345,\n",
      "        0.0157, 0.0462, 0.0436, 0.0328, 0.0225, 0.0279, 0.0257, 0.0557, 0.0265])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.08386755734682083\n",
      "log likelihood: -2.4785163402557373\n",
      "negative log likelihood: 2.4785163402557373\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 2.9907329082489014\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(4)\n",
    "for i in range(4):\n",
    "  # i-th bigram:\n",
    "  x1, x2 = xs[i][0].item(), xs[i][1].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x1]}{itos[x2]}{itos[y]} (indexes {x1},{x2},{y})')\n",
    "  print('input to the neural net:', (x1,x2))\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 54])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "num_classes = 27\n",
    "xenc = torch.cat([F.one_hot(xs[:, 0], num_classes=num_classes),\n",
    "                  F.one_hot(xs[:, 1], num_classes=num_classes)], dim=1).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(4), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8845, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backwards pass\n",
    "W.grad = None # set to zero gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196113 196113\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(words):\n",
    "        # dataset\n",
    "        xs , ys = [], []\n",
    "        for w in words:\n",
    "            chs = ['.'] + list(w) + ['.']\n",
    "            for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "                ix1 = stoi[ch1]\n",
    "                ix2 = stoi[ch2]\n",
    "                ix3 = stoi[ch3]\n",
    "                xs.append([ix1,ix2])\n",
    "                ys.append(ix3)\n",
    "\n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        num_1 = xs.shape[0]\n",
    "        num_2 = ys.nelement()\n",
    "        print(num_1, num_2)\n",
    "        return xs, ys, num_1\n",
    "\n",
    "# Model Training\n",
    "xs_train, ys_train, num_train = create_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0248,  0.5203, -1.8783,  ..., -0.2901,  0.0437, -0.4449],\n",
      "        [-0.6851, -1.2949,  1.6538,  ...,  0.2800,  0.1134,  1.2624],\n",
      "        [-0.0205, -0.6873,  0.9004,  ...,  0.3653, -1.9820,  0.8117],\n",
      "        ...,\n",
      "        [ 2.3286,  0.9181,  1.0550,  ...,  0.4955,  0.3674, -0.1687],\n",
      "        [-0.4859, -1.4310,  0.2528,  ...,  0.2638, -1.5383,  0.3591],\n",
      "        [ 0.1135, -0.4826, -0.8430,  ...,  0.5211,  0.4590, -0.7903]],\n",
      "       requires_grad=True)\n",
      "Loss at iteration 1 : 4.167153358459473\n",
      "Loss at iteration 2 : 3.487342119216919\n",
      "Loss at iteration 3 : 3.177805185317993\n",
      "Loss at iteration 4 : 2.9976861476898193\n",
      "Loss at iteration 5 : 2.875115394592285\n",
      "Loss at iteration 6 : 2.786677122116089\n",
      "Loss at iteration 7 : 2.7198877334594727\n",
      "Loss at iteration 8 : 2.66743540763855\n",
      "Loss at iteration 9 : 2.6250784397125244\n",
      "Loss at iteration 10 : 2.5901150703430176\n",
      "Loss at iteration 11 : 2.5607123374938965\n",
      "Loss at iteration 12 : 2.5355911254882812\n",
      "Loss at iteration 13 : 2.5138447284698486\n",
      "Loss at iteration 14 : 2.494824171066284\n",
      "Loss at iteration 15 : 2.4780545234680176\n",
      "Loss at iteration 16 : 2.4631781578063965\n",
      "Loss at iteration 17 : 2.449916124343872\n",
      "Loss at iteration 18 : 2.4380412101745605\n",
      "Loss at iteration 19 : 2.427366256713867\n",
      "Loss at iteration 20 : 2.4177327156066895\n",
      "Loss at iteration 21 : 2.409006118774414\n",
      "Loss at iteration 22 : 2.4010720252990723\n",
      "Loss at iteration 23 : 2.3938326835632324\n",
      "Loss at iteration 24 : 2.387204170227051\n",
      "Loss at iteration 25 : 2.381115436553955\n",
      "Loss at iteration 26 : 2.375504970550537\n",
      "Loss at iteration 27 : 2.3703203201293945\n",
      "Loss at iteration 28 : 2.36551570892334\n",
      "Loss at iteration 29 : 2.3610520362854004\n",
      "Loss at iteration 30 : 2.3568952083587646\n",
      "Loss at iteration 31 : 2.353015899658203\n",
      "Loss at iteration 32 : 2.3493874073028564\n",
      "Loss at iteration 33 : 2.345986843109131\n",
      "Loss at iteration 34 : 2.34279465675354\n",
      "Loss at iteration 35 : 2.3397929668426514\n",
      "Loss at iteration 36 : 2.3369650840759277\n",
      "Loss at iteration 37 : 2.3342978954315186\n",
      "Loss at iteration 38 : 2.331777811050415\n",
      "Loss at iteration 39 : 2.3293938636779785\n",
      "Loss at iteration 40 : 2.3271355628967285\n",
      "Loss at iteration 41 : 2.3249940872192383\n",
      "Loss at iteration 42 : 2.322960376739502\n",
      "Loss at iteration 43 : 2.3210272789001465\n",
      "Loss at iteration 44 : 2.3191871643066406\n",
      "Loss at iteration 45 : 2.3174338340759277\n",
      "Loss at iteration 46 : 2.3157615661621094\n",
      "Loss at iteration 47 : 2.3141651153564453\n",
      "Loss at iteration 48 : 2.3126399517059326\n",
      "Loss at iteration 49 : 2.311180830001831\n",
      "Loss at iteration 50 : 2.309783697128296\n",
      "Loss at iteration 51 : 2.3084447383880615\n",
      "Loss at iteration 52 : 2.3071606159210205\n",
      "Loss at iteration 53 : 2.3059282302856445\n",
      "Loss at iteration 54 : 2.304743766784668\n",
      "Loss at iteration 55 : 2.303605556488037\n",
      "Loss at iteration 56 : 2.3025102615356445\n",
      "Loss at iteration 57 : 2.301455497741699\n",
      "Loss at iteration 58 : 2.3004391193389893\n",
      "Loss at iteration 59 : 2.299459218978882\n",
      "Loss at iteration 60 : 2.298513889312744\n",
      "Loss at iteration 61 : 2.2976012229919434\n",
      "Loss at iteration 62 : 2.2967193126678467\n",
      "Loss at iteration 63 : 2.2958672046661377\n",
      "Loss at iteration 64 : 2.2950427532196045\n",
      "Loss at iteration 65 : 2.2942450046539307\n",
      "Loss at iteration 66 : 2.2934725284576416\n",
      "Loss at iteration 67 : 2.292724609375\n",
      "Loss at iteration 68 : 2.291999340057373\n",
      "Loss at iteration 69 : 2.2912962436676025\n",
      "Loss at iteration 70 : 2.290614128112793\n",
      "Loss at iteration 71 : 2.289952039718628\n",
      "Loss at iteration 72 : 2.28930926322937\n",
      "Loss at iteration 73 : 2.2886850833892822\n",
      "Loss at iteration 74 : 2.2880783081054688\n",
      "Loss at iteration 75 : 2.2874884605407715\n",
      "Loss at iteration 76 : 2.286914825439453\n",
      "Loss at iteration 77 : 2.2863566875457764\n",
      "Loss at iteration 78 : 2.285813331604004\n",
      "Loss at iteration 79 : 2.2852845191955566\n",
      "Loss at iteration 80 : 2.2847697734832764\n",
      "Loss at iteration 81 : 2.2842679023742676\n",
      "Loss at iteration 82 : 2.2837789058685303\n",
      "Loss at iteration 83 : 2.283302068710327\n",
      "Loss at iteration 84 : 2.282837390899658\n",
      "Loss at iteration 85 : 2.282383918762207\n",
      "Loss at iteration 86 : 2.2819416522979736\n",
      "Loss at iteration 87 : 2.2815101146698\n",
      "Loss at iteration 88 : 2.2810885906219482\n",
      "Loss at iteration 89 : 2.28067684173584\n",
      "Loss at iteration 90 : 2.2802748680114746\n",
      "Loss at iteration 91 : 2.2798821926116943\n",
      "Loss at iteration 92 : 2.27949857711792\n",
      "Loss at iteration 93 : 2.279123544692993\n",
      "Loss at iteration 94 : 2.2787563800811768\n",
      "Loss at iteration 95 : 2.278398036956787\n",
      "Loss at iteration 96 : 2.2780470848083496\n",
      "Loss at iteration 97 : 2.2777037620544434\n",
      "Loss at iteration 98 : 2.2773680686950684\n",
      "Loss at iteration 99 : 2.2770392894744873\n",
      "Loss at iteration 100 : 2.2767174243927\n",
      "Loss at iteration 101 : 2.276402235031128\n",
      "Loss at iteration 102 : 2.2760937213897705\n",
      "Loss at iteration 103 : 2.2757911682128906\n",
      "Loss at iteration 104 : 2.2754948139190674\n",
      "Loss at iteration 105 : 2.2752044200897217\n",
      "Loss at iteration 106 : 2.2749202251434326\n",
      "Loss at iteration 107 : 2.2746407985687256\n",
      "Loss at iteration 108 : 2.2743680477142334\n",
      "Loss at iteration 109 : 2.274099349975586\n",
      "Loss at iteration 110 : 2.273836374282837\n",
      "Loss at iteration 111 : 2.273578405380249\n",
      "Loss at iteration 112 : 2.2733254432678223\n",
      "Loss at iteration 113 : 2.2730767726898193\n",
      "Loss at iteration 114 : 2.2728328704833984\n",
      "Loss at iteration 115 : 2.2725934982299805\n",
      "Loss at iteration 116 : 2.2723586559295654\n",
      "Loss at iteration 117 : 2.272128105163574\n",
      "Loss at iteration 118 : 2.2719013690948486\n",
      "Loss at iteration 119 : 2.271678924560547\n",
      "Loss at iteration 120 : 2.2714602947235107\n",
      "Loss at iteration 121 : 2.271245241165161\n",
      "Loss at iteration 122 : 2.2710342407226562\n",
      "Loss at iteration 123 : 2.270826578140259\n",
      "Loss at iteration 124 : 2.270622968673706\n",
      "Loss at iteration 125 : 2.2704222202301025\n",
      "Loss at iteration 126 : 2.2702255249023438\n",
      "Loss at iteration 127 : 2.270031690597534\n",
      "Loss at iteration 128 : 2.269840955734253\n",
      "Loss at iteration 129 : 2.269653797149658\n",
      "Loss at iteration 130 : 2.2694692611694336\n",
      "Loss at iteration 131 : 2.2692880630493164\n",
      "Loss at iteration 132 : 2.2691097259521484\n",
      "Loss at iteration 133 : 2.2689340114593506\n",
      "Loss at iteration 134 : 2.268761396408081\n",
      "Loss at iteration 135 : 2.2685914039611816\n",
      "Loss at iteration 136 : 2.2684240341186523\n",
      "Loss at iteration 137 : 2.268259048461914\n",
      "Loss at iteration 138 : 2.268097162246704\n",
      "Loss at iteration 139 : 2.267937421798706\n",
      "Loss at iteration 140 : 2.267780065536499\n",
      "Loss at iteration 141 : 2.267624855041504\n",
      "Loss at iteration 142 : 2.267472743988037\n",
      "Loss at iteration 143 : 2.267322301864624\n",
      "Loss at iteration 144 : 2.267174005508423\n",
      "Loss at iteration 145 : 2.267028331756592\n",
      "Loss at iteration 146 : 2.2668845653533936\n",
      "Loss at iteration 147 : 2.2667429447174072\n",
      "Loss at iteration 148 : 2.266603469848633\n",
      "Loss at iteration 149 : 2.2664661407470703\n",
      "Loss at iteration 150 : 2.2663304805755615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # initializing the network\n",
    "W = torch.randn((54,27), generator=g, requires_grad=True)\n",
    "print(W)\n",
    "\n",
    "# # gradient descent\n",
    "for k in range(150):\n",
    "    # forward pass\n",
    "    xenc = torch.cat([F.one_hot(xs_train[:,0], num_classes=27),\n",
    "                        F.one_hot(xs_train[:,1], num_classes=27)],dim=1).float()\n",
    "    logits = xenc @ W # log-counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num_train), ys_train].log().mean() + 0.01*(W**2).mean() # nll + regularisation\n",
    "    print(f\"Loss at iteration {k+1} : {loss}\")\n",
    "\n",
    "    # backwards pass\n",
    "    W.grad = None # set to zero gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -40 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3845,  2.4490, -0.2068,  ..., -1.2826,  0.6379,  0.3838],\n",
       "        [ 1.3693,  0.9898,  0.0648,  ..., -1.3017,  0.2562,  0.0183],\n",
       "        [ 0.2546,  1.5845,  0.2096,  ...,  0.0942,  0.6071, -0.1656],\n",
       "        ...,\n",
       "        [ 1.2108,  0.6588,  0.5350,  ...,  1.2588,  0.5984,  0.1321],\n",
       "        [ 1.6412,  1.5217, -0.7105,  ..., -0.8067, -1.8990, -0.1341],\n",
       "        [ 0.4080,  1.7619, -0.9265,  ..., -0.0640,  1.2845,  0.0047]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lye.\n",
      "rieynna.\n",
      "ana.\n",
      "quilikardyna.\n",
      "xa.\n",
      "elaistoratenthylironeleen.\n",
      "breylesip.\n",
      "pre.\n",
      "wishob.\n",
      "vebexi.\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = random.randint(1,26)\n",
    "    out.append(itos[ix2])\n",
    "    # ix2 = 0\n",
    "    while True:\n",
    "        xenc = torch.cat([F.one_hot(torch.tensor([ix1]), num_classes=27),\n",
    "                    F.one_hot(torch.tensor([ix2]), num_classes=27)],dim=1).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        \n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
