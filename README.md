# CharLM

## Index of Implementations
This repository contains multiple implementations of character-level language models:

1. [Count-Based Trigram Language Model](#1-count-based-trigram-language-model)  
   A statistical model that calculates probabilities using character trigram counts with smoothing.  

2. [Neural Network-Based Trigram Language Model](#2-neural-network-based-trigram-language-model)  
   A neural network implementation of a trigram-based language model inspired by Bengio et al. (2003).  

3. [Advanced Neural Network Character-Level Language Model](#3-advanced-neural-network-character-level-language-model)  
   A more flexible and customizable deep learning model with character embeddings and multilayer perceptrons (MLPs).  


## 1. Trigram Language Model

### Overview
This is the first part of `CharLM`, trigram-based character-level language model designed to generate sample names and evaluate probabilities of sequences in a dataset. The model supports **two training methods**:
1. **Count-based Trigram Model**: A statistical model based on counts of character trigrams.
2. **Neural Network-based Trigram Model**: A deep learning model trained using one-hot encoding of character pairs and a linear transformation.

The model generates synthetic names based on learned probabilities and evaluates the dataset using **negative log likelihood**.


### Installation and Usage

#### Dependencies
The code requires the following libraries:
- Python 3.8+
- PyTorch
- argparse

Install PyTorch using:
```bash
pip install torch
```

#### How to Run
1. **Prepare a Text File**:
   - Create a text file where each line represents a word or name (e.g., `names.txt`).

2. **Run the Model**:
   Use the following command to train and evaluate the model:
   ```bash
   python charlm.py --file_path <path_to_file> --method <count/neural> --epochs <num_epochs> --lr <learning_rate>
   ```

   **Example**:
   ```bash
   python charlm.py --file_path names.txt --method neural --epochs 200 --lr 50
   ```

### Arguments
| Argument               | Description                                                                                  | Default       |
|----------------|----------------------------------------------------------------------------------------------|---------------|
| `--file_path`          | Path to the text file containing the dataset.                                                | Required      |
| `--method`             | Training method: `count` for count-based or `neural` for neural network-based training.       | `count`       |
| `--epochs`              | Number of epochs (only for neural network-based training).                                    | `200`         |
| `--lr`                  | Learning rate (only for neural network-based training).                                      | `50`          |

---

### Model Details

#### 1. Count-Based Trigram Model
- **How It Works**:
  - Counts the frequency of each trigram (sequence of three characters).
  - Calculates probabilities with **smoothing** to handle unseen trigrams.
- **Loss Function**:
  - Negative log likelihood of the dataset.

#### 2. Neural Network-Based Trigram Model
- **How It Works**:
  - Uses one-hot encoding of character pairs as input.
  - Learns a weight matrix (`W`) through gradient descent.
- **Loss Function**:
  - Negative log likelihood with L2 regularization.

---

### Output
1. **Training, Validation, and Test Loss**:
   - Displays performance metrics for the model.

2. **Generated Names**:
   - Outputs a list of 10 sample names generated by the trained model.

**Example Output**:
```
--------------------------------
Count Based Training
Train Loss: 2.5634
Val Loss: 2.7104
Test Loss: 2.6851
Sample names generated by the model:
Albina
Bert.
Cora.
Delta.
...
```

---

### 2. Neural Network Character-Level Language Model

#### Overview
This is a character-level language model that uses a **deep neural network** inspired by the paper "A Neural Probabilistic Language Model" by Bengio et al. (2003) to generate character sequences (e.g., names or words). The model is trained using a dataset of words where each line contains a sample, learning character embeddings and sequence patterns to predict the next character in a sequence to generate synthetic samples and evaluate the model's performance on training, validation, and test datasets.

#### Features
1. **Character Embeddings**:
   - Learns embeddings for each character in the dataset.
   - Embedding dimension is adjustable using the `--emd_dim` argument.

2. **Customizable Network**:
   - Two-layer neural network with a hidden layer size adjustable using `--neurons_second_layer`.

3. **Sequence Prediction**:
   - Predicts the next character based on a sequence of prior characters.

4. **Synthetic Data Generation**:
   - Generates words based on the trained model using sampling.

---

### Installation and Usage

#### Dependencies
Ensure the following libraries are installed:
- Python 3.8+
- PyTorch
- argparse

Install PyTorch using:
```bash
pip install torch
```

#### How to Run
1. **Prepare the Dataset**:
   - Create a text file (`dataset.txt`) where each line represents a single word.

2. **Run the Model**:
   Train the neural network using:
   ```bash
   python mlp_lang_model.py --file_path <path_to_dataset> [options]
   ```

   **Example**:
   ```bash
   python mlp_lang_model.py --file_path dataset.txt --epochs 100000 --batch_sz 64 --blr 0.01 --emd_dim 16 --block_sz 5 --neurons_second_layer 128 --samples 10
   ```

### Arguments
| Argument               | Description                                                                                  | Default       |
|-------------------------|----------------------------------------------------------------------------------------------|---------------|
| `--file_path`           | Path to the text file containing the dataset.                                                | Required      |
| `--epochs`              | Number of epochs to train the model.                                                         | `200000`      |
| `--batch_sz`            | Batch size used for training.                                                                | `32`          |
| `--blr`                 | Base learning rate used for training.                                                       | `0.1`         |
| `--decay`               | Decay rate for learning rate in the second phase of training.                                | `0.1`         |
| `--emd_dim`             | Dimension of the embedding vectors.                                                         | `10`          |
| `--block_sz`            | Context window size (number of previous characters to use).                                  | `3`           |
| `--neurons_second_layer`| Number of neurons in the second layer of the neural network.                                 | `200`         |
| `--samples`             | Number of samples to be generated after training.                                            | `20`          |

---

### Model Details

#### Architecture
1. **Embedding Layer**:
   - Maps each character to a vector of size `emd_dim`.
   - Captures semantic relationships between characters.

2. **Hidden Layer**:
   - A fully connected layer with `neurons_second_layer` neurons.
   - Uses the **tanh** activation function.

3. **Output Layer**:
   - Produces logits for each character in the vocabulary.
   - Uses a softmax function for probability distribution.

#### Loss Function
- The model uses **Cross-Entropy Loss** to measure the prediction error.

---

### Output
1. **Training and Validation Loss**:
   - Displays the loss at the end of training and validation.

2. **Generated Samples**:
   - Outputs synthetic samples generated by the model.

**Example Output**:
```
Training Started
Number of parameters in the model: 17697
Step: 0 || Loss: 32.61125564575195
Step: 10000 || Loss: 2.3048486709594727
Step: 20000 || Loss: 2.3219010829925537
...
Step: 190000 || Loss: 1.9214112758636475
Final Train Loss: 2.0580263137817383
Final Dev Loss: 2.3804244995117188
Time taken for training: 297.1182
jeralynna.
dareylia.
kozalanie.
serenna.
...
```

---

### Paper Reference
This implementation is inspired by the foundational work:
> Bengio, Yoshua, et al. *"A Neural Probabilistic Language Model."* Journal of Machine Learning Research 3 (2003): 1137-1155.

## Contributions
Feel free to contribute by improving the loss functions, adding new features, or testing on additional datasets.
